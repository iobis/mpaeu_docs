[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MPA Europe - OBIS contribution documentation",
    "section": "",
    "text": "1 Introduction to the OBIS contribution to MPA Europe\nThe EU Horizon project “Marine Protected Areas Europe (MPA Europe, https://mpa-europe.eu) aims to identify the locations within the European seas where MPAs would protect the highest number of species, habitats and ecosystems. This information is crucial to establish a functional MPA network and will help managers to propose further areas for conservation in the future. Innovative and bold, MPA Europe will also go one step further by considering the potential blue carbon benefits of the prioritization.\nOBIS (Ocean Biodiversity Information System) will contribute with three pieces of information which will be supplied to the area prioritization process: (i) species distribution models (SDMs) of at least half of the European marine species; (ii) diversity metrics for European seas; and (iii) habitat maps considering habitat forming species. In all cases, models will be created using OBIS data and will include predictions for future scenarios according to CMIP6."
  },
  {
    "objectID": "index.html#about-this-documentation",
    "href": "index.html#about-this-documentation",
    "title": "MPA Europe - OBIS contribution documentation",
    "section": "1.1 About this documentation",
    "text": "1.1 About this documentation\nTransparency and reproducibility are an integral part of our contribution to the project. Thus, here you will find included all the details about the approaches and decisions we took in each part of the work. This is a complement to the available codes/data."
  },
  {
    "objectID": "studyarea.html",
    "href": "studyarea.html",
    "title": "2  Defining the study area",
    "section": "",
    "text": "Our first effort was in establishing a consistent study area shape that could be used by all the work package teams. Using as a reference the study area portrayed in the MPA Europe technical description, we defined the boundaries of the final shape from a product developed by the Flanders Marine Institute that intersects the Exclusive Economic Zones (EEZ) and the IHO (International Hydrographic Organization) seas map.\n\n\n\nThe study area\n\n\nThe whole process of creating the study area, as well as the produced shapefiles are available at the GitHub repository mpaeu_studyarea."
  },
  {
    "objectID": "datadownload.html#selecting-species-for-modeling",
    "href": "datadownload.html#selecting-species-for-modeling",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.1 Selecting species for modeling",
    "text": "3.1 Selecting species for modeling\nThe first step was to establish the species occurring on the study area. We used the robis function dataset to retrieve all species occurring on the study area. Further on we filtered the dataset to:\n\nretain only taxa at the species level\nretain only taxa with accepted taxonomic status\nremove Archaea, Bacteria, Fungi and Protozoa taxa\ninclude only marine or brackish species\n\nFrom that we obtained a final list of 22159 species.\nIn the case of GBIF, we first downloaded all data occurring within our study area. Then, using the worrms package we verified which of those species were marine. Then we performed the same filters used with the OBIS data. This resulted in 22782 unique species."
  },
  {
    "objectID": "datadownload.html#downloading-data",
    "href": "datadownload.html#downloading-data",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.2 Downloading data",
    "text": "3.2 Downloading data\nOBIS data was obtained from the full export available at https://obis.org/data/access/. However, code for downloading the data through the robis package is available. This is done through the obissdm package (which is being developed to support this project)\nGBIF data was downloaded using the rgbif package, via the obissdm package."
  },
  {
    "objectID": "datadownload.html#quality-control-steps-under-development",
    "href": "datadownload.html#quality-control-steps-under-development",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.3 Quality control steps (under development)",
    "text": "3.3 Quality control steps (under development)\n\n3.3.1 Duplicate records removal\nWe removed duplicated data points using GeoHash with a precision of 6 (width ≤ 1.22km X height 0.61km), and the year. Thus, for each combination of GeoHash cell and year, only one record was kept. That part is implemented in the mp_dup_check function, of the project package msdm.\n\n\n\n\n\n\nNote\n\n\n\nWe note that, specifically for the SDMs, before modeling we do an additional duplicate removal in order to keep only a single record per cell.\n\n\n\n\n3.3.2 Remove records on land\nRecords on land were removed based on openmap.\n\n\n\n\n\n\nNote\n\n\n\nWe further filtered the records for the SDMs by keeping only those overlapping the environmental variable layers (which present some differences to the land layer used before).\n\n\n\n\n3.3.3 Geographical and environmental outliers (flagging)\nFor the assessment of geographical outliers we implemented an innovative method that considers the existence of barriers when calculating the distance between points. Usually, geographical outliers are calculated based on the cartesian distance between the points. However, for marine species (indeed, also for terrestrial ones) the barriers are important because it constrains dispersal. Consider for example one species on the two sides of the Panama strait (Atlantic and Pacific). A straight line between the two points would be a short distance. However, if we take in account the barrier, then the animal (or its larvae) would need to travel a much longer distance to reach the other side of the strait.\nIn both the geographical and the environmental (Sea Surface Temperature, bathymetry and salinity) outlier assessment, we used a threshold of 3 times the inter quantile range to identify extreme points, but we just flagged the most extreme outlier until a limit of 1% of the points (i.e. if there were more points above the threshold, just the most extreme ones were flagged)."
  },
  {
    "objectID": "datamining.html",
    "href": "datamining.html",
    "title": "4  Obtaining additional biodiversity data",
    "section": "",
    "text": "5 Codes for obtaining information from data repositories"
  },
  {
    "objectID": "datamining.html#obisdi-package",
    "href": "datamining.html#obisdi-package",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.1 obisdi package",
    "text": "4.1 obisdi package\nFor enabling a streamlined and standard ingestion of data throughout the project we developed the obisdi package, which is available on GitHub. The idea behind the package (and basically all the structure) came from the Tracking Invasive Alien Species (TrIAS) project’ checklist recipe (see more here), which provides a standard structure for mapping data to the Darwin Core standard. Using this structure, all the mapping is fully documented and can be tracked. Also, it’s possible to directly ingest the data to the IPT from a GitHub repository.\nEvery project created with the obisdi package have the following structure:\n\na folder for data, containing two other folders - one for raw data (where the original data files are stored) and one for processed data (where the final edited files are stored).\na README file containing the basic details about the dataset and the repository\nan RMarkdown file which contains the mapping to the DwC standard.\n\nBy knitting the RMarkdown files, it’s also possible to generate a docs folder that can be used as a website (through GitHub pages), providing an easy access information for the general community.\n\n\n\nWorkflow for data ingestion using obisdi"
  },
  {
    "objectID": "datamining.html#additional-data-from-literature-and-repositories",
    "href": "datamining.html#additional-data-from-literature-and-repositories",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.2 Additional data from literature and repositories",
    "text": "4.2 Additional data from literature and repositories\n\n4.2.1 BioTIME\nBioTIME is a database containing time series of ecological data from the terrestrial, freshwater and marine realm. We downloaded the full database (available here) and using the metadata information we identified those marine studies (on the Europe region) which were not available on OBIS. This identification procedure was based on a fuzzy matching of the titles with the OBIS dataset titles. For those that were probably relevant, we manually checked the datasets to confirm its relevance.\nAt the end we identified 4 new datasets that could be included, and proceeded with the data ingestion.\n\n\n\n\n\n\nWarning\n\n\n\nAt this moment, only one of the identified datasets was already ingested. The others are under processing and will soon be ingested.\n\n\n\n\n4.2.2 Literature\nWe searched on Web of Science for articles that could potentially contain datasets valuable for our project. We used the following search string: TS=((marine OR ocean* OR coastal) AND ((“biodiversity data”) OR (dataset) OR (“time series” OR time-series)) AND (species OR occurrence OR biodiversity OR fauna) AND (europe* OR global)). From the returned list (~2000 articles) we (1) matched the titles with the dataset names or bibliographic citations from OBIS to verify if the dataset was already included on OBIS, and (2) screened (manually) to identify if the dataset was valuable. Note that this is not a systematic review, but an exploratory search. Because the number of records was considerably large and the screening involves evaluating the data quality and the methods that generated it, in this first phase of the project we screened the first 100 records (ordered by relevance), and will keep screening in the following months.\n\n\n4.2.3 Data repositories\nWe searched the data repositories FigShare, Zenodo, and Dryad for datasets linked with marine data on the region of our study. For each of those repositories, a distinct search strategy was applied, based on their structure. Dataset names were fuzzy matched with dataset titles on OBIS and those identified as not available on OBIS were screened to assess its relevance. In this first phase of the project we screened the first 50 records, and will keep screening in the following months.\nOnce one dataset is identified for inclusion, it will be ingested using the obisdi structure.\nCodes for obtaining the information from those data repositories are available on the last section.\n\n\n4.2.4 Other sources\nWe also received suggestions of datasets directly from the participants of the project. We checked if the suggested dataset was not already on OBIS and, if not, we ingested the dataset."
  },
  {
    "objectID": "datamining.html#additional-data-from-gbif",
    "href": "datamining.html#additional-data-from-gbif",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.3 Additional data from GBIF",
    "text": "4.3 Additional data from GBIF\nAfter we obtained the list of species occurring on the study area, we downloaded the occurrence data from GBIF. From the occurrence data, we identified the unique datasets from which the data came from. We then counted the number of data each dataset contributed to the final data. We selected those datasets that had a high contribution of data (more than 50000 occurrences) as potential datasets that could be included in OBIS.\nFor the datasets with potential for inclusion, we first identified those that are already part of OBIS and excluded them from the search. With the remaining datasets, we screened for relevance.\nThe harvesting of the datasets to OBIS is done with the contribution and approval of an OBIS node. To do that, we follow this procedure:\n\nAn issue is open on the GitHub repo https://github.com/iobis/obis-network-datasets, indicating the dataset\nOne of the OBIS nodes will review the issue and verify the relevance and quality of the dataset\nIf the dataset is deemed valuable, then the OBIS node approves it and its harvested to the OBIS dataset.\n\n\n\n\n\n\n\nNote\n\n\n\nOnly datasets with CC0, CC-BY or CC-BY-NC license were considered for inclusion. More information on the OBIS manual."
  },
  {
    "objectID": "datamining.html#zenodo",
    "href": "datamining.html#zenodo",
    "title": "4  Obtaining additional biodiversity data",
    "section": "5.1 Zenodo",
    "text": "5.1 Zenodo\n\n# Get records from Zenodo using API connection\n\n# Create a function to retrieve the records for a certain query\nget_zenodo &lt;- function(query){\n  response &lt;- httr::GET('https://zenodo.org/api/records',\n                        query = list(q = query,\n                                     size = 2000, page = 1))\n  \n  t_resp &lt;- httr::content(response, \"parsed\", encoding = \"UTF-8\")\n  \n  results &lt;- lapply(t_resp, function(x){\n    data.frame(title = x$title, doi = x$doi)\n  })\n  \n  results &lt;- do.call(\"rbind\", results)\n  \n  return(results)\n  \n}\n\nzen_results &lt;- get_zenodo(\"+access_right:open +resource_type.type:dataset +title:marine +title:species\")\n\nwrite.csv(zen_results, paste0(\"source_lists/zen_\", format(Sys.Date(), \"%d%m%Y\"), \".csv\"),\n          row.names = F)"
  },
  {
    "objectID": "datamining.html#figshare",
    "href": "datamining.html#figshare",
    "title": "4  Obtaining additional biodiversity data",
    "section": "5.2 FigShare",
    "text": "5.2 FigShare\n\n# Get records from FigShare using API connection\nlibrary(httr)\n\n# Create a function to retrieve the records for a certain query\nquery_fig &lt;- '{\n  \"item_type\": 3,\n  \"search_for\": \"(:title: marine OR :title: ocean OR :title: coastal) AND (:title: europe OR :title: global) AND (:title: species OR :title: biodiversity)\",\n  \"limit\": 1000,\n  \"offset\": 0\n}'\n\nget_figshare &lt;- function(query, maxtry = 7000){\n  \n  off &lt;- seq(0, maxtry, by = 1000)\n  \n  retnum &lt;- 1000\n  \n  k &lt;- 1\n  \n  allres &lt;- list()\n  \n  while(retnum == 1000 & k &lt;= length(off)) {\n    \n    query &lt;- gsub('\"offset\": [[:digit:]]*', paste0('\"offset\": ', off[k]), query)\n    \n    response &lt;- POST(\"https://api.figshare.com/v2/articles/search\", body=query, \n                     httr::add_headers(`accept` = 'application/json'), \n                     httr::content_type('application/json'))\n    \n    if (response$status_code != 200) {\n      results &lt;- data.frame(title = NA, doi = NA, resource_title = NA)\n      retnum &lt;- 1000\n    } else {\n      t_resp &lt;- httr::content(response, \"parsed\", encoding = \"UTF-8\")\n      \n      results &lt;- lapply(t_resp, function(x){\n        data.frame(title = x$title, doi = x$doi, resource_title = x$resource_title)\n      })\n      \n      results &lt;- do.call(\"rbind\", results)\n      \n      retnum &lt;- nrow(results)\n    }\n    \n    allres[[k]] &lt;- results\n    \n    k &lt;- k + 1\n    \n  }\n  \n  return(allres)\n  \n}\n\nfig_q1 &lt;- get_figshare(query_fig)\n\n# Bind all results\nfig_results &lt;- do.call(\"rbind\", fig_q1)\n\nwrite.csv(fig_results, paste0(\"source_lists/fig_\", format(Sys.Date(), \"%d%m%Y\"), \".csv\"),\n          row.names = F)"
  },
  {
    "objectID": "datamining.html#dryad",
    "href": "datamining.html#dryad",
    "title": "4  Obtaining additional biodiversity data",
    "section": "5.3 Dryad",
    "text": "5.3 Dryad\n\n# Get records from Dryad using API connection\nlibrary(httr)\n\n# Create a function to retrieve the records for a certain query\nget_dryad &lt;- function(query, maxtry = 2000, addstop = T, verbose = T){\n  \n  off &lt;- seq(1, ceiling(maxtry/100))\n  \n  retnum &lt;- 100\n  \n  k &lt;- 1\n  \n  allres &lt;- list()\n  \n  while(retnum == 100 & k &lt;= length(off)) {\n    \n    if (verbose) cat(\"Downloading page\", k, \"\\n\")\n    \n    response &lt;- httr::GET('https://datadryad.org/api/v2/search',\n                          query = list(q = query,\n                                       per_page = 100, page = k))\n    \n    if (response$status_code != 200) {\n      results &lt;- data.frame(title = NA, doi = NA, resource_title = NA)\n      retnum &lt;- 100\n    } else {\n      t_resp &lt;- httr::content(response, \"parsed\", encoding = \"UTF-8\")\n      \n      results &lt;- lapply(t_resp$`_embedded`$`stash:datasets`, function(x){\n        id &lt;- x$identifier\n        title &lt;- x$title\n        if (is.null(title)) {\n          title &lt;- \"NOT FOUND\"\n        }\n        data.frame(title = title, doi = id)\n      })\n      \n      results &lt;- do.call(\"rbind\", results)\n      \n      retnum &lt;- nrow(results)\n    }\n    \n    allres[[k]] &lt;- results\n    \n    k &lt;- k + 1\n    \n    if (addstop) {\n      Sys.sleep(5)\n    }\n    \n  }\n  \n  return(allres)\n  \n}\n\ndry_q1 &lt;- get_dryad(\"marine species europe\")\ndry_q1 &lt;- do.call(\"rbind\", dry_q1)\n\ndry_q2 &lt;- get_dryad(\"marine species global\")\ndry_q2 &lt;- do.call(\"rbind\", dry_q2)\n\n# Bind all results\ndry_results &lt;- rbind(dry_q1, dry_q2)\n\nwrite.csv(dry_results, paste0(\"source_lists/dry_\", format(Sys.Date(), \"%d%m%Y\"), \".csv\"),\n          row.names = F)"
  },
  {
    "objectID": "datamining.html#code-for-fuzzy-matching-from-data-repositories",
    "href": "datamining.html#code-for-fuzzy-matching-from-data-repositories",
    "title": "4  Obtaining additional biodiversity data",
    "section": "5.4 Code for fuzzy matching from data repositories",
    "text": "5.4 Code for fuzzy matching from data repositories\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(sf)\n\nzen &lt;- read.csv(\"source_lists/zen_23062023.csv\")\ndry &lt;- read.csv(\"source_lists/dry_23062023.csv\")\nfig &lt;- read.csv(\"source_lists/fig_23062023.csv\")\n\nfull &lt;- rbind(\n  zen[,c(\"title\", \"doi\")],\n  dry[,c(\"title\", \"doi\")],\n  fig[,c(\"title\", \"doi\")]\n)\n\n# Get OBIS datasets\n# Open study area shapefile\nstarea &lt;- st_read(\"~/Research/mpa_europe/mpaeu_studyarea/data/shapefiles/mpa_europe_starea_v2.shp\")\nstarea &lt;- st_bbox(starea)\n\n# Download list of all obis datasets in the study area\ndatasets &lt;- robis::dataset(\n  geometry = st_as_text(st_geometry(st_as_sfc(st_bbox(starea))))\n)\n\n#### PYTHON IMPLEMENTATION\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python3\")\n\nfuz &lt;- import(\"rapidfuzz\")\n\nsources &lt;- tolower(full$title)\n\ncompare &lt;- tolower(datasets$title)\n\nmatch_frat &lt;- match_title &lt;- rep(NA, length(sources))\n\ncli::cli_progress_bar(\"Running fuzzy matching...\", total = length(sources))\n\nfor (s in 1:length(sources)) {\n  frat &lt;- rep(NA, length(compare))\n  \n  for (z in 1:length(compare)) {\n    frat[z] &lt;- fuz$fuzz$ratio(sources[s], compare[z])\n  }\n  \n  match_title[s] &lt;- compare[which.max(frat)]\n  match_frat[s] &lt;- max(frat, na.rm = T)\n  \n  cli::cli_progress_update()\n}\ncli::cli_progress_done()\n\ncross_check &lt;- full\ncross_check$match_titles &lt;- match_title\ncross_check$fuzzy_ratio &lt;- match_frat\n#### END OF PYTHON IMPLEMENTATION\n\n# Save for external edition\nwrite_csv(cross_check, \"final_lists/datarepo_datasets_comparison.csv\")"
  }
]