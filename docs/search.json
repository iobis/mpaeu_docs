[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MPA Europe - OBIS contribution documentation",
    "section": "",
    "text": "1 Introduction to the OBIS contribution to MPA Europe\nThe EU Horizon project “Marine Protected Areas Europe (MPA Europe, https://mpa-europe.eu) aims to identify the locations within the European seas where MPAs would protect the highest number of species, habitats and ecosystems. This information is crucial to establish a functional MPA network and will help managers to propose further areas for conservation in the future. Innovative and bold, MPA Europe will also go one step further by considering the potential blue carbon benefits of the prioritization.\nOBIS (Ocean Biodiversity Information System) will contribute with three pieces of information which will be supplied to the area prioritization process: (i) species distribution models (SDMs) of at least half of the European marine species; (ii) diversity metrics for European seas; and (iii) habitat maps considering habitat forming species. In all cases, models will be created using OBIS data and will include predictions for future scenarios according to CMIP6."
  },
  {
    "objectID": "index.html#about-this-documentation",
    "href": "index.html#about-this-documentation",
    "title": "MPA Europe - OBIS contribution documentation",
    "section": "1.1 About this documentation",
    "text": "1.1 About this documentation\nTransparency and reproducibility are an integral part of our contribution to the project. Thus, here you will find included all the details about the approaches and decisions we took in each part of the work. This is a complement to the available codes/data."
  },
  {
    "objectID": "studyarea.html",
    "href": "studyarea.html",
    "title": "2  Defining the study area",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "datadownload.html#selecting-species-for-modeling",
    "href": "datadownload.html#selecting-species-for-modeling",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.1 Selecting species for modeling",
    "text": "3.1 Selecting species for modeling\nThe first step was to establish the species occurring on the study area. We used the robis function dataset to retrieve all species occurring on the study area. Further on we filtered the dataset to:\n\nretain only taxa at the species level\nretain only taxa with accepted taxonomic status\nremove Archaea, Bacteria, Fungi and Protozoa taxa\ninclude only marine or brackish species\n\nFrom that we obtained a final list of 27112 species."
  },
  {
    "objectID": "datadownload.html#downloading-data",
    "href": "datadownload.html#downloading-data",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.2 Downloading data",
    "text": "3.2 Downloading data\nOBIS data was obtained from the full export available at https://obis.org/data/access/.\nGBIF data was downloaded through the rgbif package. We limited the number of downloaded records to 30000 [TO VERIFY IF COULD BE CIRCUMVENTED]."
  },
  {
    "objectID": "datadownload.html#quality-control-steps",
    "href": "datadownload.html#quality-control-steps",
    "title": "3  Obtaining occurrence data from OBIS and GBIF",
    "section": "3.3 Quality control steps",
    "text": "3.3 Quality control steps\n\n3.3.1 Duplicate records removal\nWe removed duplicated data points using GeoHash with a precision of 6 (width ≤ 1.22km X height 0.61km), and the year. Thus, for each combination of GeoHash cell and year, only one record was kept. That part is implemented in the mp_dup_check function, of the project package msdm.\n\n\n\n\n\n\nNote\n\n\n\nWe note that, specifically for the SDMs, before modeling we do an additional duplicate removal in order to keep only a single record per cell.\n\n\n\n\n3.3.2 Remove records on land\nRecords on land were removed based on [TODO - see if use GHRSS or openmap].\n\n\n\n\n\n\nNote\n\n\n\nWe further filtered the records for the SDMs by keeping only those overlapping the environmental variable layers (which present some differences to the land layer used before).\n\n\n\n\n3.3.3 Geographical outliers (flagging)\nThe novelty of our approach is that for computing the distance between points we considered the existence of barriers. This is specially important for marine species. So for example, the distance between a point on the Pacific side of the Panama strait, and on the Atlantic side, would be XXX when considering only the cartesian distance. However, when we consider the barrier, the distance is of XXX.\n\n\n3.3.4 Environmental outliers (flagging)"
  },
  {
    "objectID": "datamining.html",
    "href": "datamining.html",
    "title": "4  Obtaining additional biodiversity data",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "datamining.html#obisdi-package",
    "href": "datamining.html#obisdi-package",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.1 obisdi package",
    "text": "4.1 obisdi package\nFor enabling a streamlined and standard ingestion of data throughout the project we developed the obisdi package, which is available on GitHub. The idea behind the package (and basically all the structure) came from the Tracking Invasive Alien Species (TrIAS) project’ checklist recipe (see more here), which provides a standard structure for mapping data to the Darwin Core standard. Using this structure, all the mapping is fully documented and can be tracked. Also, it’s possible to directly ingest the data to the IPT from a GitHub repository.\nEvery project created with the obisdi package have the following structure:\n\na folder for data, containing two other folders - one for raw data (where the original data files are stored) and one for processed data (where the final edited files are stored).\na README file containing the basic details about the dataset and the repository\nan RMarkdown file which contains the mapping to the DwC standard.\n\nBy knitting the RMarkdown files, it’s also possible to generate a docs folder that can be used as a website (through GitHub pages), providing an easy access information for the general community."
  },
  {
    "objectID": "datamining.html#additional-data-from-literature-and-repositories",
    "href": "datamining.html#additional-data-from-literature-and-repositories",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.2 Additional data from literature and repositories",
    "text": "4.2 Additional data from literature and repositories\n\n4.2.1 BioTIME\n\n\n4.2.2 Literature databases\nWeb of Science\nTS=((marine OR ocean* OR coastal) AND ((“biodiversity data”) OR (dataset) OR (“time series” OR time-series) AND (species OR occurrence OR biodiversity OR fauna)))\nGoogle Scholar\n\n\n4.2.3 Data repositories\nZenodo\nDryad\nFigShare"
  },
  {
    "objectID": "datamining.html#additional-data-from-gbif",
    "href": "datamining.html#additional-data-from-gbif",
    "title": "4  Obtaining additional biodiversity data",
    "section": "4.3 Additional data from GBIF",
    "text": "4.3 Additional data from GBIF\nAfter we obtained the list of species occurring on the study area, we downloaded the occurrence data from GBIF. From the occurrence data, we identified the unique datasets from which the data came from. We then counted the number of data each dataset contributed to the final data. We selected those datasets that had a high contribution of data as potential datasets that could be included in OBIS.\nFor the datasets with potential for inclusion, we first identified those that are already part of OBIS and excluded them from the search. With the remaining datasets, we ran a code to identify the number of marine species within the dataset. All datasets with at least 50% of marine species were tagged as potentially relevant for inclusion on OBIS.\nThe harvesting of the datasets to OBIS is done with the contribution and approval of an OBIS node. To do that, we follow this procedure:\n\nAn issue is open on the GitHub repo https://github.com/iobis/obis-network-datasets, indicating the dataset\nOne of the OBIS nodes will reveal the issue and verify the relevance and quality of the dataset\nIf the dataset is deemed valuable, then the OBIS node approves it and its harvested to the OBIS dataset."
  }
]