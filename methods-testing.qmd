---
bibliography: references.bib
---

# Testing SDM methods

::: {.callout-note title="Under development"}
This section is still under development.
:::

The **Deliverable 2** of the **WP3** aims to produce species distribution models (SDM) for at least half of all marine species occurring in the study area of the MPA Europe project. As a first step, we tested which modeling approach would be better suited for our research problem. Specifically, we sought a model that would:

-   Perform effectively under various conditions, including low and high sampling scenarios
-   Exhibit sufficient flexibility to work in conditions when a careful individual tuning is unattainable (for example, we will not be able to chose *a priori* the better environmental layers for each species, but only for broad groups)
-   Avoid or minimize overfitting
-   Have short fitting and prediction time

Previous studies have already explored the performance of SDM algorithms and parameters [@marmion2008; @inman2021; @barbet-massin2012; @valavi2022]. Worth of note, @valavi2021 evaluated the performance of several presence-only methods under a point process approach, the same framework we are adopting here. Therefore, our tests were not aimed at conducting an exhaustive examination of SDM parameterization. Instead, building upon the aforementioned research, we concentrated on identifying the optimal settings for our specific modeling challenge.

In special, a very sensitive point for our study was the speed of fitting and prediction, given the necessity to fit a substantial number of models.

## Chosen algorithms

Based on @valavi2021 and in our previous experience, we selected the following algorithms to test:

-   GLM (Generalized Linear Models)
-   LASSO (LASSO Regularized GLM, through `glmnet`)
-   Maxent (Maximum Entropy algorithm, through `maxnet` implementation)
-   BRT (Boosted Regression Trees)
-   RandomForest

We also considered for very experimental purposes one other implementation of Boosted Regression Trees (also called GBM - Gradient Boosting Model), the LightGBM. This machine learning method performs very well in machine learning tasks and is much faster than other GBM implementations. However, as it was designed for large datasets it can suffer from overfitting.

GLMs were fitted with four variations: no weighting, naive-weighting, IWLR (Infinitely Weighted Linear Regression) and DWPR (Down-Weighted Poisson Regression). BRT were fitted with no weight and with naive-weighting. Random Forest were fitted with no-weighted classification and down-sampled classification.

Algorithms were implemented as modules in the [`obissdm` package](https://github.com/iobis/mpaeu_msdm). This package was specifically designed to hold the primary functions utilized in our modeling framework, thus facilitating more comprehensive documentation.

## Test with virtual species

For the first part of the SDM methods testing, we created 2 virtual species (in which we have full control of the generative process; for more information on virtual species see @leroy2015 and @zurell2010). Because we know the true distribution of the virtual species, we can better explore the differences in performance of the distinct algorithms under distinct conditions [@meynard2019]. We performed a set of testings considering the following scenarios:

-   Two conditions of environmental data: all environmental variables known (species 101) and one variable unknown (Gaussian noise) (species 103)
-   Two sampling conditions: with bias and without bias
-   Two data availability conditions: low (30 points) and high (150 points)

This sums up to 8 modeling settings. Because the sampling process of points is random, we generated 10 samples of presence points for each species (yielding a total 80 models to be fitted). We also generated 10 samples (for each species) of presence-absence points to be used as independent evaluation datasets.

## Model fitting

### Environmental data

Environmental data for both the present period and for two future scenarios for the year 2100 (SSP2 and SSP5) were obtained from the new version of Bio-ORACLE (not published yet, but see @assis2018) using the package `biooracler`. Specifically, for producing and fitting the models of the virtual species we used the following variables (all for surface):

-   Sea Temperature
-   Salinity
-   Phosphate
-   Total Phytoplankton
-   Bathymetry

All variables were cropped to the study area, but considering a buffer to expand it. Variables were masked to a bathymetry of -1500m

### Quadrature sampling

Quadrature points were randomly sampled in a number of 150000. We chose this number using a Down-Weighted Poisson Regression over different numbers of quadrature points [@renner2015; @el-gabbas2018]. The chosen number was enough to likelihood convergence in all species.

### Model tuning and evaluation

For model tuning and evaluation we used a spatial block cross-validation strategy, with 5 folds. Spatial block cross-validation is better suited for assessing spatial models, as it ensures that testing data will show at least a certain degree of environmental difference from the information used to train the model [@roberts2017]. We tested two types of spatial blocks: grid blocks and latitudinal bins. In the first, a grid of squares with a certain size are defined over the study area and randomly assigned into folds, while in the second, the study area is divided in a fixed number of latitudinal bins which are randomly assigned into folds (e.g. @assis2023).

The size of the block can be defined to reflect the spatial correlation between variables [@valavi2018]. However, for this test we defined a fixed block size for all tests to improve the fitting speed. The size (5 degrees) is deemed large enough to provide environmental variability to the testing data.

Models were evaluated using AUC (Area Under the Receiver-Operating-Characteristic Curve) and CBI (Continuous Boyce Index), which are better suited for presence-only models. We also obtained TSS (True Skill Statistics) for three distinct thresholds (Maximum Sensitivity + Specificity, P10 and Minimum Training Presence). Final predictions for both the current and future period were compared with the original species suitable area using the I statistic of niche overlap, with a higher niche overlap meaning that the model better reflected the original condition. Binary maps produced with the P10 threshold were compared with the binary occurrence maps of the virtual species using Jaccard.

### Results

In general, all models had a good performance, except for the classification Random Forest. Forest based methods are know to have problems with class imbalance, a problem solved with the down-sampled implementation. Some methods had an excellent performance both for model evaluation (AUC, CBI) and capacity of predicting a similar distribution than the true one (I, Jaccard metrics): LASSO, Maxent and BRT naive. Indeed, LASSO and Maxent were expected to behavior similarly as under the hood `maxnet` uses LASSO.

![AUC metrics for the virtual species testing.](images/auc_metrics_vsp.png){fig-align="center"}

![CBI metrics for the virtual species testing. Values below 0 indicates counter prediction (low values where presence was expected).](images/tss_metrics_vsp.png){fig-align="center"}

![TSS metrics for the virtual species testing.](images/tss_metrics_vsp.png){fig-align="center"}

![I metrics for the virtual species testing.](images/i_metrics_vsp.png){fig-align="center"}

![I metrics for the virtual species testing considering the SSP1 scenario.](images/i_metrics_ssp1_vsp.png){fig-align="center"}

![Jaccard metrics for the virtual species testing.](images/jacc_metrics_vsp.png){fig-align="center"} 

![Example showing the real distribution in the current period and the predicted distribution by 4 distinct models. Warmer colors are higher relative occurrence rate.](images/map_current.png){fig-align="center"}

![Example showing the real distribution in the future period (SSP1 - 2100) and the predicted distribution by 4 distinct models. Warmer colors are higher relative occurrence rate.](images/map_future.png){fig-align="center"}

From those models with best performance, LASSO was the one with fastest tuning-prediction time, although with very close timings to Maxent.

![TSS metrics for the virtual species testing.](images/timings_vsp.png){fig-align="center"}

## Test with real species

For the second part of the SDM methods testing, we selected 4 fish species occurring in our study area for which we had a good number of points coverage and, thus, a better knowledge of the current distribution of the species. We also chose those species because IUCN range maps are available and thus we could assess how well our models agree with the range maps (although we acknowledge that IUCN range maps can have important bias).

We tested in this part 4 algorithms that scored better with the virtual species test: LASSO, Maxent, BRT and RF Down-sampled.

Model fitting followed the same methods described for the virtual species test. Occurrence data for the species were obtained from both OBIS and GBIF. To reduce bias, only 1 occurrence point per cell was kept. Ideally, this record pruning would also consider the spatial autocorrelation and points would be pruned in a distance that reduces such correlation (see for one application). However, calculating the spatial autocorrelation for each species that will be modeled can introduce a significant additional time. Thus, for those tests we considered two simpler strategies (although we will try one other approach in the future): (1) keeping 1 point per cell in the same resolution of the environmental variables and (2) keeping 1 point per cell in a grid with a coarsened resolution (the double of the environmental layers, 0.1 degrees).

We chose a set of environmental variables to be used with all 5 species: sea temperature, salinity, phosphate, total phytoplankton and sea water velocity. The correlation level of those variables is low, as assessed through VIF (Variance Inflation Factor) using the function `vifstep` with a threshold of 6 (package `usdm`, @naimi2013). Despite the same set of variables was used to all species, the type of the variable varied according to the species' mode of life (e.g. bottom temperature for benthic species). Data for the mode of life was obtained through an automated search in three databases: Sealifebase, Fishbase, and WoRMS. The search was done in this order, so that if the information was found in multiple sources, the first one was used. In case the information was not found for a particular species, the function tried to find the mode of life of the parent level. The function was also developed in a way to be conservative. So, if a species was registered as both benthic and pelagic, we used variables for benthic environment.

For benthic, demersal or pelagic species associated with the bottom we set variables describing conditions in the maximum depth. Mesopelagic species would be modeled with conditions for the mean depth, while both pelagic and species for which no information was found were set to be modeled with surface conditions.

### Results

Again, all models in general performed well, although the Maxent CBI result for _Pagrus auriga_ was poor (counter prediction).

```{r echo=FALSE, caption="Standard points metrics"}
library(kableExtra)
std <- read.csv("files/standard_pts_metrics.csv")
colnames(std) <- c("Species", "Model", "AUC", "CBI")
x <- knitr::kable(std, caption = "Metrics for distinct fish species")
column_spec(x, 1, italic = TRUE)
```

Aggregating the points to a coarser resolution had a small effect in the results.

```{r echo=FALSE, caption="Aggregated points metrics"}
agg <- read.csv("files/aggregated_pts_metrics.csv")
colnames(agg) <- c("Species", "Model", "AUC", "CBI")
y <- knitr::kable(agg, caption = "Aggregated (coarser resolution) points metrics for distinct fish species")
column_spec(y, 1, italic = TRUE)
```

Models in general reflected the spatial distribution of the occurrence points well.

![Distribution model of _Pagrus pagrus_, according to LASSO and RF down-sampled. IUCN range map and occurrence points are shown on the first image.](images/map_pagrus.png){fig-align="center"}

None of the models showed high levels of agreement with the IUCN maps. This discrepancy could be caused by issues related to the expert range maps. However, it might also highlight a potential difficulty in selecting a method to binarize the models. In this study, we opted for the P10 threshold, which determines the value at which 90% of the presence points would be classified as 'present'. This represents a relatively conservative approach, but alternative methods may yield better results.

```{r echo=FALSE, caption="Aggregated points metrics"}
jac <- read.csv("files/iucn_jacc_res.csv")
colnames(jac) <- c("Species", "Model", "Jaccard")
y <- knitr::kable(jac, caption = "Jaccard metric between binary distribution map (p10 threshold) and IUCN range maps for different models. STD refers to those modeled with the standard 1 point per cell, while AGG refers to the aggregated version.")
column_spec(y, 1, italic = TRUE)
```

_NOTE: BRT model failed for **Pagrus auriga** when aggregated._

## Overview and chosen approach

Based on the findings presented here, it is suggested that both LASSO and Maxnet are viable options. Additionally, BRT and RF Down-weighted could be considered as potential candidates. Since LASSO and Maxnet are closely related, we recommend using LASSO due to its faster implementation.

::: {.callout-warning title="Availability of codes"}
The codes for this modeling exercise are available on the GitHub repository [iobis/mpaeu_sdm](https://github.com/iobis/mpaeu_sdm). However, the data is not available due to size limitations on GitHub. Soon this data will be available in other repository.
:::

## References
